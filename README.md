# RESTFUL service with Lambda function
#### Name: Amey Kasbe
#### UIN: 674285381
#### Email: akasbe2@uic.edu

## Project Description
The intention of the project is to analyse log data generated by [LogFileGenerator](https://github.com/ameykasbe/LogGeneratorHW3).

### Job 1
Deploy an instance of the log file generation program on EC2 and configure it to run for some period of time producing and storing log messages into log file in some S3-allocated storage.

### Job 2
To develop an algorithm of at most O(log N) complexity for locating messages from a set of N messages that contain some designated pattern-matching string that are timestamped within some delta time interval from the requested time. Create a lambda function with the algorithm that returns appropriate message with count of the log messages with 400 or 200-level HTTP client response.

### Job 3
Invoke the lambda functions by using the AWS API Gateway with the requests GET and POST and DELETE.


## Job1
### Changes in log generator
* The log generator given in the project description is - https://github.com/0x1DOCD00D/LogFileGenerator
* The log generator used to develop the solution is - https://github.com/ameykasbe/LogGeneratorHW3
* The changes are in `logback-test.xml`.
  * In the log events the time of the log event now contains date as well - "yyyy-MM-dd HH:mm:ss.SSS" 
  * Changed the name of the log file to - LogFile.log

### Setup

**Only those steps are mentioned that requires to select non-default values**

1. Create a EC2 instance
   1. Sign in to AWS 
      1. Navigate to AWS EC2
      2. Click on Launch instance
         1. Select "Amazon Linux 2 AMI (HVM), SSD Volume Type"
         2. Download the key-pair in the process
2. Create IAM role for EC2 to give permission to access S3 buckets
   1. Navigate to AWS IAM Permissions 
      1. Select roles from the left panel
      2. Click on create roles 
         1. Click on EC2 in common cases 
         2. Search for "AmazonS3FullAccess"
         3. Create role
3. Attach the created IAM role to the instance
   1. Navigate to EC2 instance
   2. Click on actions -> Security -> Modify IAM roles
   3. Select the newly created role
   4. Click on save
4. Login to the EC2 instance
   1. Open terminal in the local setup
   2. Copy the permission file downloaded in step 1 of this guide to root directory
   3. Give access permission to the file
   chmod 400 <.pem_file>
   
   4. Copy the Public DNS of the instance from AWS portal.
      1. Connect -> SSH client -> The example is the public DNS
   5. Paste and enter it into the terminal
   6. Become the root user of the EC2 instance by command <br />
         `sudo su`
5. Install Java, SBT and Git into the EC2 instance <br />
   1. Java
   `wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.rpm` <br />
   `rpm -ivh jdk-17_linux-x64_bin.rpm` <br />
   2. SBT
  `curl -L https://www.scala-sbt.org/sbt-rpm.repo > sbt-rpm.repo` <br />
  `sudo mv sbt-rpm.repo /etc/yum.repos.d/` <br />
  `yum install sbt` <br />
   3. Git 
   `yum install git` <br />
6. Clone the git repository
   `git clone https://github.com/ameykasbe/LogGeneratorHW3.git`

7. Compile and test run
   `sbt clean compile` <br />
   `sbt clean compile run` <br />
8. Create a S3 bucket.
9. Setup cron job
   1. Create cron script
      1. In the root directory create a cron script e.g. `cronscript.sh`
      2. Enter three commands
         1. Move to the log generator directory
         2. Generate logs
         3. Copy the log file into S3 bucket <br />
         Example - `cd /root/gRPC-REST-lambda/; sbt clean compile run && aws s3 cp /root/gRPC-REST-lambda/log/LogFile.log s3://ameykasbe-cs441-hw3-bucket;`
   2. Give access permission to the file
   `chmod +x cronscript.sh`
   3. Setup cron job
      1. Navigate to root directory
      2. Open crontab. <br />
         `crontab -e`
      3. Enter the cron job details. For example to generate logs everyday at 6 PM - 
        `00 18 * * * /root/cronscript.sh`
      
      
## Job2
### [Lambda Function](https://aws.amazon.com/lambda/)
* Lambda function is a serverless compute service which can be triggered by some events. 
### Algorithm
* Lambda function can be found in the directory lambda/ with name `lambda_grpc.py`
* The algorithm used is Binary Search.
  * The first log event's position becomes the high pointer
  * The last log event's position becomes the low pointer.
  * The log is divided into half (figuratively) in every iteration and is checked if the time of the middle log event (with pointer mid) falls between the required duration
  * The log is continuously divided until the any log is found with time inside the required time duration
  * From that log event, logs are traversed in both directions until the log arrives which are not in the required time duration.
  * While traversal, the log message is searched for required pattern, if it is found, a counter (initialized as 0) is increased.
  * At last the output with proper status code and message are sent as response.
* Status codes
  * `200` - Pattern present
  * `404` - Pattern not present. OR Time duration requested is not in the log file.
* Configuration parameters are passed as request parameters.

### [AWS API Gateway](https://aws.amazon.com/api-gateway/)
* AWS API Gateway is a service that makes it easier to create and maintain APIs.

### Setup AWS Lambda and AWS API Gateway

1. Setup AWS Lambda function 
   1. Navigate to AWS Lambda
   2. Create Function
      1. Select from scratch
         1. Enter name 
         2. Select Python as language
         3. Enter the code from lambda/lambda_grpc.py
         4. Deploy
2. Setup API gateway
   1. Navigate to AWS Lambda function
   2. Click on Add Trigger
   3. Select API Gateway
   4. Create an API
      1. Configuration
         1. API Type - REST
         2. Security - Open
         
* Gateway is now open to internet
* Lambda function will be triggered by HTTP GET and POST requests.
3. Create IAM role for Lambda to give permission to access S3 buckets
   1. Navigate to IAM Permissions
   2. Select roles from the left panel
   3. Click on Create role
      1. Click on Lambda in common cases
      2. Search for "AmazonS3FullAccess"
      3. Create role
4. Attach the created IAM role to the Lambda
    1. Navigate to the Lambda function
    2. Click on Configuration -> Permission -> Execution role
    3. Click on Edit under Execution roles
    4. Select the role name given above e.g. LAMBDA-S3-TEST

5. Test the lambda function with format given in lambda/ directory.


## Job3
### Important files

### Execution Process
### Versions
* [Java 8](https://www.oracle.com/java/technologies/downloads/#java8-windows) - Used while developing the project.
* Scala version 2.12.4. This is important because Akka 2.5.26 used while developing the project is supported by Scala 2.12.X. Compatibilities can be checked and different versions can also be used as per the compatibility matrix.

### Use appropriate configurations
* Check `applications.conf` for all the configurations.
* Configurations are explained in the comments.

### Local Environment Execution
#### Setup Hadoop Environment
* Load the HortonBox sandbox image in vmWare Workstation pro
* Start the virtual machine
* Access the Hadoop environment by HortonBox Ambari environment using web browser. The IP Address can be found in the VM.
* Access the VM using Putty with same IP address. Credentials would be hadoop/hadoop. You will be prompted to change the credentials.
* Create admin account on Ambari by command - <br />
`ambari-admin-password-reset`

#### Execute jar file
* Upload the jar file into the HDFS using Ambari dashboard interface.
* Access the VM using Terminal, GET the jar file into the VM using the command - <br />
`hadoop fs -get /path_to_jar/`
* Execute the jar file for EACH job -<br />
`hadoop jar <fileName>.jar Execution <jobNumber> <pathToInput> <pathToOutput>`

#### Output files
* Output files can be accessed inside the path given as argument. For each job a _jobN directory will be created.

## AWS Elastic MapReduce Execution
### Setup
* Sign In to Amazon Web Services

#### AWS EC2 (Elastic Compute Cloud)
* Navigate to AWS EC2
* On EC2 dashboard, on the left Panel search for Key-Pair
* Create a Key-Pair

#### AWS S3 (Simple Storage Service)
* Navigate to AWS S3
* Create an S3 bucket
* Upload the jar and log files in the S3 bucket created.

#### AWS EMR (Elastic MapReduce)
* Navigate to AWS EMR
* Create an EMR cluster 
  * Enter a name
  * Select the S3 bucket created earlier
  * Select the Key-Pair created earlier
* Wait till the cluster is ready
* Navigate to Steps tab under AWS EMR
* Add steps for individual jobs
  * Click on Add Step
  * Select the jar from AWS S3 uploaded earlier
  * Provide arguments in format <br />
    `Execution <JobNumber> <s3BucketInputLocation> <s3BucketOutputLocation>`
    Examples -  
  * Job 1 <br />
    `Execution 1 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output`
  * Job 2 <br />
    `Execution 2 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output`
  * Job 3 <br />
    `Execution 3 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output`
  * Job 4 <br />
    `Execution 4 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output`

## YouTube Link
* YouTube link of the short video that documents all steps of the deployment and execution -
  [YouTube Link](https://youtu.be/DLTfhaKWtB4)


## Results
### Job 1
Create a file in comma separated format with distribution of message types in logs with messages of a particular pattern across predefined time interval.


![](etc/Job1.png)

* For the log file present in the etc directory, the output is as shown in the screenshot.
* The first column represents log message types.
* The second column represents the number of times log events have appeared in the logs for that particular message type with given pattern and between the time duration defined in configuration.
* The logic behind this MapReduce job is that if we can find the log events in mappers which fall under the time gap mentioned in the configuration file, which also contains the pattern, we can send the multiset of Message Type as Key and 1 as value from the mapper to reducer.
* Then, in reducers, where after shuffling we get Message type as key and multiset of 1, which are the number of instances of such log events of such message type, we can take the sum of all the 1s and get the sum of message types.




### Job 2
Time intervals sorted in the descending order that contained most log messages of the type ERROR with injected regex pattern string instances.

![](etc/Job2.png)

* For the log file present in the etc directory, the output is as shown in the screenshot.
* * The first column represents time intervals from intialTime to EndTime.
* The second column represents the number of times log events have appeared in the logs for ERROR message type with given pattern and between the time interval.
* The logic behind this is the output from mapper should be a multiset of key value pair where key is the interval and value is 1, which represents the one time the ERROR message type has occurred in the time interval.
* The reducer receives input in key value pair such that key is the time interval and value is multiset of 1s. Computing the sum of elements of value will result in the total number of ERROR messages in the time interval in the log.




### Job 3
Create a file in comma separated format with distribution of message types in logs with messages of a particular pattern.

![](etc/Job3.png)

* For the log file present in the etc directory, the output is as shown in the screenshot.
* The first column represents log message types.
* The second column represents the number of times log events have appeared in the logs for that particular message type with given pattern defined in configuration.
* The logic behind this MapReduce job is that if we can find the log events in mappers which also the pattern defined in the configuration, we can send the multiset of Message Type as Key and 1 as value from the mapper to reducer.
* Then, in reducers, where after shuffling we get Message type as key and multiset of 1, which are the number of instances of such log events of such message type, we can take the sum of all the 1s and get the sum of message types.



### Job 4
Create a file in comma separated format with number of characters in each log message for each log message type that contain the highest number of characters in the detected instances of the designated regex pattern.

![](etc/Job4.png)

* For the log file present in the etc directory, the output is as shown in the screenshot.
* The first column represents log message types.
* The second column represents the maximum number of character of message in the log event, for that particular message type with given pattern  defined in configuration.
* The logic behind this MapReduce job is that if we can find the log events in mappers which contains the pattern, we can send the multiset of Message Type as Key and string length of the message as value from the mapper to reducer.
* Then, in reducers, where after shuffling we get Message type as key and multiset of string length of messages, we can take the maximum of all the string lengths and get the sum of message types.

## Unit testing procedure
### Using IntelliJ Idea
1. Clone this repository
2. Import the project in IntelliJ Idea
3. Run the `DistributedProcessingTestSuite` class from `DistributedProcessingTestSuite.scala`.

### By SBT test command
1. Clone this repository
2. Ensure necessary dependencies are installed
    * Java 8
    * Scala runtime
    * SBT
   etc.
3. In terminal, navigate to root path 
4. Execute - <br />
`sbt clean compile test`

## References
1. Dr. Grechanik, Mark, (2020) Cloud Computing: Theory and Practice.
2. [Apache Hadoop](http://hadoop.apache.org/)
3. [HortonBox](https://www.cloudera.com/downloads/hortonworks-sandbox.html)
4. [VMware Workstation Pro](https://www.vmware.com/products/workstation-pro.html)
5. [MapReduce Wikipedia](https://en.wikipedia.org/wiki/MapReduce)
6. http://indico.ictp.it/event/8170/session/10/contribution/21/material/0/1.pdf
7. [vmWare Wikipedia](https://en.wikipedia.org/wiki/VMware_Workstation)
8. [Edureka](https://www.youtube.com/c/edurekaIN) 
9. [TutorialsPoint](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm)